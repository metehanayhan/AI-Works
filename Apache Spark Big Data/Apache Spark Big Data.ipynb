{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVf9dzOBEW-_"
   },
   "source": [
    "# Metehan Ayhan - ApacheSpark, Big Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "P8ogNGxUDZfY"
   },
   "outputs": [],
   "source": [
    "#pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0W1BggTvDpXp",
    "outputId": "ff01507c-b1ba-4df0-ef56-f0fc67768755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 16, 3136, 49, 16, 4]\n"
     ]
    }
   ],
   "source": [
    "# PySpark için gerekli modülleri içe aktarıyoruz\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "# SparkContext oluşturuluyor, Spark uygulaması burada çalışacak\n",
    "sc = SparkContext()\n",
    "\n",
    "# Bir RDD (Resilient Distributed Dataset) oluşturuluyor, paralel olarak işlenecek veri\n",
    "rdd = sc.parallelize([3, 4, 56, 7, 4, 2])\n",
    "\n",
    "# RDD üzerindeki her elemanı karesi alınarak yeni bir RDD oluşturuluyor\n",
    "sq = rdd.map(lambda x: x * x)\n",
    "\n",
    "# RDD'deki veriler toplanarak bir liste halinde yazdırılıyor\n",
    "print(sq.collect())\n",
    "\n",
    "# SparkContext sonlandırılıyor, tüm kaynaklar serbest bırakılıyor\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0442r77FzhO",
    "outputId": "17858f22-4e27-4b32-c9d4-211aa6c2b158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554\t5.36F\n",
      "EZE00100082\t7.70F\n"
     ]
    }
   ],
   "source": [
    "# Bu proje, 1800.csv dosyasındaki hava durumu verilerini analiz eder ve her hava durumu istasyonunda kaydedilen\n",
    "# en düşük sıcaklıkları bulur. PySpark kullanarak verilerin filtrelenmesi, dönüştürülmesi ve azaltılması gibi temel\n",
    "# büyük veri işlemlerini gösteren bir örnektir.\n",
    "\n",
    "# PySpark için gerekli modülleri içe aktarıyoruz\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "# Spark uygulaması için bir SparkConf nesnesi oluşturuluyor\n",
    "# 'local' olarak çalışacağı belirtiliyor ve uygulama adı 'MinTemperatures' olarak ayarlanıyor\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MinTemperatures\")\n",
    "\n",
    "# SparkContext oluşturuluyor, Spark uygulaması burada çalışacak\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# Her satırı parçalayıp gerekli bilgileri (istasyon ID'si, giriş türü, sıcaklık) döndüren bir fonksiyon tanımlıyoruz\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]  # İstasyon kimliği\n",
    "    entryType = fields[2]  # Giriş türü (TMIN, TMAX, vs.)\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0  # Santigrat'tan Fahrenheit'a sıcaklık dönüşümü\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "# '1800.csv' dosyasındaki veriler bir RDD olarak yükleniyor\n",
    "lines = sc.textFile(\"1800.csv\")\n",
    "\n",
    "# Satırlar parseLine fonksiyonu kullanılarak parçalanıyor\n",
    "parsedLines = lines.map(parseLine)\n",
    "\n",
    "# Yalnızca minimum sıcaklık girişlerini (TMIN) filtreliyoruz\n",
    "minTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1])\n",
    "\n",
    "# İstasyon kimliği ve sıcaklık çiftleri oluşturuyoruz\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "\n",
    "# Aynı istasyon için kaydedilen en düşük sıcaklıkları buluyoruz\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\n",
    "\n",
    "# Sonuçları topluyoruz\n",
    "results = minTemps.collect()\n",
    "\n",
    "# Her istasyonun kimliği ve en düşük sıcaklık değeri ekrana yazdırılıyor\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
    "\n",
    "# SparkContext sonlandırılıyor, tüm kaynaklar serbest bırakılıyor\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RV6OSz-II8mH",
    "outputId": "7b9b7597-6956-416a-cb8a-dac8c8279913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554\t90.14F\n",
      "EZE00100082\t90.14F\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MaxTemperatures\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "lines = sc.textFile(\"1800.csv\")\n",
    "parsedLines = lines.map(parseLine)\n",
    "minTemps = parsedLines.filter(lambda x: \"TMAX\" in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: max(x,y))\n",
    "results = minTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHfonFwNKXDP",
    "outputId": "8218361c-e21a-40f0-d245-d91ee75cd279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'CAPTAIN AMERICA' is the most popular superhero, with 1933 co-appearances.\n"
     ]
    }
   ],
   "source": [
    "# Bu proje, bir çizgi roman veri setindeki süper kahramanların birlikte yer aldıkları sayıları analiz eder\n",
    "# ve en çok ortak görünüme sahip süper kahramanı bulur. PySpark kullanarak büyük veri işlemleri,\n",
    "# eşlemeler ve azaltma işlemleri gibi temel veri işleme tekniklerini gösteren bir örnektir.\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Bir satırdaki süper kahramanların sayısını ve bu süper kahramanın ID'sini döndüren bir fonksiyon tanımlıyoruz\n",
    "def countCoOccurences(line):\n",
    "    elements = line.split()\n",
    "    return (int(elements[0]), len(elements) - 1)\n",
    "\n",
    "# Süper kahraman isimlerini ve ID'lerini döndüren bir fonksiyon tanımlıyoruz\n",
    "def parseNames(line):\n",
    "    fields = line.split('\\\"')\n",
    "    return (int(fields[0]), fields[1].encode(\"utf8\"))\n",
    "\n",
    "# Spark uygulaması için bir SparkConf nesnesi oluşturuluyor\n",
    "# 'local' olarak çalışacağı belirtiliyor ve uygulama adı 'PolpularHero' olarak ayarlanıyor\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"PolpularHero\")\n",
    "\n",
    "# SparkContext oluşturuluyor, Spark uygulaması burada çalışacak\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# 'Marvel-names.txt' dosyasındaki süper kahraman isimleri bir RDD olarak yükleniyor\n",
    "names = sc.textFile(\"Marvel-names.txt\")\n",
    "namesRdd = names.map(parseNames)\n",
    "\n",
    "# 'Marvel-graph.txt' dosyasındaki süper kahraman ilişkileri bir RDD olarak yükleniyor\n",
    "lines = sc.textFile(\"Marvel-graph.txt\")\n",
    "\n",
    "# Her satırdaki süper kahraman ID'si ve bu kahramanın birlikte yer aldığı diğer kahraman sayısını içeren çiftler oluşturuluyor\n",
    "pairings = lines.map(countCoOccurences)\n",
    "\n",
    "# Her süper kahraman için birlikte göründüğü kahramanların toplam sayısı hesaplanıyor\n",
    "totalFriendsByCharacter = pairings.reduceByKey(lambda x, y : x + y)\n",
    "\n",
    "# Toplam arkadaş sayısını anahtar olarak kullanarak, ID ile birlikte sıralama için çiftler oluşturuluyor\n",
    "flipped = totalFriendsByCharacter.map(lambda xy : (xy[1], xy[0]))\n",
    "\n",
    "# En çok arkadaşa sahip süper kahraman bulunuyor\n",
    "mostPopular = flipped.max()\n",
    "\n",
    "# Bu süper kahramanın ismi, ID'si kullanılarak bulunuyor\n",
    "mostPopularName = namesRdd.lookup(mostPopular[1])[0]\n",
    "\n",
    "# En popüler süper kahramanın ismi ve toplam arkadaş sayısı ekrana yazdırılıyor\n",
    "print(str(mostPopularName) + \" is the most popular superhero, with \" + str(mostPopular[0]) + \\\n",
    "      \" co-appearances.\")\n",
    "\n",
    "# SparkContext sonlandırılıyor, tüm kaynaklar serbest bırakılıyor\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmZmOxKNKXGv",
    "outputId": "65da2639-fe81-406c-a2de-96bcd4ac085d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8546265328874024\n"
     ]
    }
   ],
   "source": [
    "# Bu proje, Pima Kızılderililerinin diyabet verisini kullanarak bir lojistik regresyon modeli eğitir ve test eder.\n",
    "# Projede PySpark kullanılarak veri işleme, model eğitimi, tahmin ve değerlendirme işlemleri yapılmaktadır.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Adım 1: Spark oturumu başlatılıyor\n",
    "# SparkSession, Spark SQL ve DataFrame API'si ile çalışmak için temel nesnedir\n",
    "spark = SparkSession.builder.appName(\"PimaIndianClassification\").getOrCreate()\n",
    "\n",
    "# Adım 2: Veri kümesini yükleme\n",
    "# Pima Kızılderililerine ait diyabet veri seti CSV formatında yükleniyor\n",
    "# 'inferSchema=True' ile veri tipleri otomatik olarak belirleniyor ve 'header=True' ile ilk satır başlık olarak kabul ediliyor\n",
    "data = spark.read.csv(\"pima-indians-diabetes.csv\", inferSchema=True, header=True)\n",
    "\n",
    "# Adım 3: Eğitim için veriyi hazırlama\n",
    "# Model eğitimi için gerekli özellik sütunlarını bir araya getiriyoruz\n",
    "feature_columns = data.columns[:-1]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Özellik sütunları birleştiriliyor ve 'features' adlı yeni bir sütun oluşturuluyor\n",
    "# Sonrasında sadece 'features' ve 'Outcome' (etiket) sütunları seçiliyor\n",
    "data = assembler.transform(data).select(\"features\", \"Outcome\")\n",
    "\n",
    "# Adım 4: Veriyi eğitim ve test kümelerine ayırma\n",
    "# Veriyi rastgele olarak %70 eğitim ve %30 test olacak şekilde ikiye ayırıyoruz\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Adım 5: Lojistik regresyon modeli eğitimi\n",
    "# Lojistik regresyon modeli oluşturuluyor, etiket ve özellik sütunları belirleniyor\n",
    "lr = LogisticRegression(labelCol=\"Outcome\", featuresCol=\"features\")\n",
    "\n",
    "# Model, eğitim verisi üzerinde eğitiliyor\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Adım 6: Test verisi üzerinde tahmin yapma\n",
    "# Eğitimden sonra model, test verisi üzerinde tahmin yapıyor\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Adım 7: Modelin değerlendirilmesi\n",
    "# Modelin performansı, binary sınıflandırma için değerlendiriliyor\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Outcome\")\n",
    "\n",
    "# Modelin doğruluğu hesaplanıyor ve ekrana yazdırılıyor\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
